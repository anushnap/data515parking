{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get packages\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df1 = pd.read_csv('Annual_Parking_Study_Data.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elmntkey: 78\n",
      "Study_Area: 26\n",
      "Sub_Area: 13180\n",
      "Date Time: 0\n",
      "Side: 39\n",
      "Unitdesc: 26\n",
      "Peak Hour_SDOT: 150574\n",
      "Parking_Spaces: 298\n",
      "Total_Vehicle_Count: 485\n",
      "Dp_Count: 42687\n",
      "Rpz_Count: 131222\n",
      "TG_Car2Go: 131437\n",
      "BMW_DN: 152367\n",
      "Lime: 165552\n",
      "Idling: 163714\n",
      "Field Notes: 151822\n",
      "Construction: 13\n",
      "Event Closure: 13\n",
      "Subarea Label: 0\n",
      "Study Year: 26\n",
      "Peak Hour? (Yes or No): 0\n",
      "RPZ Blocks: 78103\n",
      "CSM: 77895\n",
      "Time Stamp: 83590\n"
     ]
    }
   ],
   "source": [
    "def printna():\n",
    "    cols = df1.columns\n",
    "    for col in cols:\n",
    "        num_nans = df1[col].isna().sum()\n",
    "        print(col + \": \" + str(num_nans))\n",
    "        \n",
    "printna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where Unitdesc (the street name) is missing\n",
    "df1.dropna(axis = 0,subset=['Unitdesc'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elmntkey: 52\n",
      "Study_Area: 0\n",
      "Sub_Area: 13180\n",
      "Date Time: 0\n",
      "Side: 13\n",
      "Unitdesc: 0\n",
      "Peak Hour_SDOT: 150548\n",
      "Parking_Spaces: 272\n",
      "Total_Vehicle_Count: 459\n",
      "Dp_Count: 42661\n",
      "Rpz_Count: 131196\n",
      "TG_Car2Go: 131411\n",
      "BMW_DN: 152341\n",
      "Lime: 165526\n",
      "Idling: 163688\n",
      "Field Notes: 151796\n",
      "Construction: 13\n",
      "Event Closure: 13\n",
      "Subarea Label: 0\n",
      "Study Year: 0\n",
      "Peak Hour? (Yes or No): 0\n",
      "RPZ Blocks: 78077\n",
      "CSM: 77895\n",
      "Time Stamp: 83564\n"
     ]
    }
   ],
   "source": [
    "printna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NE 41ST ST BETWEEN 11TH AVE NE AND 12TH AVE NE\n",
      "nan\n",
      "{39329.0: 'N', 39330.0: 'S'}\n"
     ]
    }
   ],
   "source": [
    "# For those blocks missing a \"Side\" parameter, look up other entries for those blocks to see if we can infer which side it is\n",
    "blocks_missing_side = df1['Unitdesc'][df1['Side'].isna()].unique()\n",
    "for blk in blocks_missing_side:\n",
    "    print(blk)\n",
    "    this_block = df1[df1['Unitdesc']==blk]\n",
    "    side_options = this_block['Side'].unique()\n",
    "    #side_options = side_options[~np.isnan(side_options)]\n",
    "    \n",
    "    side_dict = {}\n",
    "    for side in side_options:\n",
    "        if not isinstance(side, str):\n",
    "            print(side)\n",
    "            continue\n",
    "        else:\n",
    "            this_side = this_block[this_block['Side']==side]\n",
    "            side_dict[this_side['Elmntkey'].unique()[0]] = side\n",
    "print(side_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to fill in the missing Side entries\n",
    "df1.loc[df1['Side'].isna(),'Side'] = df1['Elmntkey'][df1['Side'].isna()].map(side_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {'Peak Hour_SDOT':'None','Parking_Spaces':0,'Total_Vehicle_Count':0,'Dp_Count':0,'Rpz_Count':0,'TG_Car2Go':0,'BMW_DN':0,'Lime':0,'Idling':0}\n",
    "df1.fillna(value=values,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elmntkey: 52\n",
      "Study_Area: 0\n",
      "Sub_Area: 13180\n",
      "Date Time: 0\n",
      "Side: 0\n",
      "Unitdesc: 0\n",
      "Peak Hour_SDOT: 0\n",
      "Parking_Spaces: 0\n",
      "Total_Vehicle_Count: 0\n",
      "Dp_Count: 0\n",
      "Rpz_Count: 0\n",
      "TG_Car2Go: 0\n",
      "BMW_DN: 0\n",
      "Lime: 0\n",
      "Idling: 0\n",
      "Field Notes: 151796\n",
      "Construction: 13\n",
      "Event Closure: 13\n",
      "Subarea Label: 0\n",
      "Study Year: 0\n",
      "Peak Hour? (Yes or No): 0\n",
      "RPZ Blocks: 78077\n",
      "CSM: 77895\n",
      "Time Stamp: 83564\n"
     ]
    }
   ],
   "source": [
    "printna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_csv('Annual_Parking_Study_Data_Cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following Unitdesc's are missing Elmntkey values entirely, both in SODO area\n",
    "# 6TH AVE S BETWEEN S BAYVIEW ST AND S LANDER ST (E side)\n",
    "# 6TH AVE S BETWEEN S STACY ST AND S BAYVIEW ST (E side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some calculated columns to our dataframe\n",
    "\n",
    "# Utilization: Total_vehicle_count/Parking_spaces\n",
    "# If Parking_spaces = 0, this will come out to Inf, so it should be 1 (or 100% instead)\n",
    "# Free spaces: Parking_spaces - Total_vehicle_count\n",
    "# Sometimes Total vehicle count exceeds parking spaces, so it should be 0 in that case\n",
    "# Some Total vehicle counts are negative and that doesn't make sense, assume typo\n",
    "df1.loc[df1['Total_Vehicle_Count']<0,'Total_Vehicle_Count'] = df1.loc[df1['Total_Vehicle_Count']<0,'Total_Vehicle_Count']*(-1)\n",
    "df1['Utilization'] = df1['Total_Vehicle_Count']/df1['Parking_Spaces']\n",
    "df1.loc[df1['Parking_Spaces']==0,'Utilization'] = 1\n",
    "df1['Free_Spaces'] = df1['Parking_Spaces'] - df1['Total_Vehicle_Count']\n",
    "df1.loc[df1['Free_Spaces']<0,'Free_Spaces'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['South Lake Union' '12th Avenue' 'SODO' 'Ballard Locks'\n",
      " 'Ballard Locks Summer' 'Ballard Locks summer' 'Pioneer Square'\n",
      " 'Cherry Hill' 'Uptown' 'First Hill' 'Commercial Core - Retail'\n",
      " 'Commercial Core - Waterfront' 'Commercial Core - Financial' 'Fremont'\n",
      " 'Uptown Triangle' 'Green Lake' 'Capitol Hill' 'Westlake' 'Pike-Pine'\n",
      " 'Commercial Core Retail' 'Commercial Core Financial'\n",
      " 'Commercial Core Waterfront' 'Commercial Core' 'Belltown'\n",
      " 'University District' 'Ballard' 'Roosevelt' 'Chinatown/ID'\n",
      " 'Ballard Locks Spring' 'Pioneer Square 2017' 'Denny Triangle'\n",
      " '12th Ave 2017 Annual Study' 'Chinatown ID 2017'\n",
      " 'Ballard Locks Spring 2017' 'Denny Triangle South - 2017'\n",
      " 'Denny Triangle North - 2017' 'Greenlake' 'Denny Triangle South'\n",
      " 'Denny Triangle North' '12th Ave - Weekday' 'Ballard - Weekday'\n",
      " 'First Hill - Weekday' 'Ballard Locks - Weekday (Spring)'\n",
      " 'Belltown - Weekday' 'Ballard Locks - Weekday (Summer)'\n",
      " 'Capitol Hill - Weekday' 'Chinatown/ID - Event' 'Cherry Hill - Weekday'\n",
      " 'Chinatown/ID - Sunday' 'Denny Triangle - Weekday'\n",
      " 'Chinatown/ID - Weekday' 'Commercial Core - Event Day'\n",
      " 'Commercial Core - Weekday' 'Fremont - Weekday' 'Little Saigon - Weekday'\n",
      " 'Green Lake - Weekday' 'Little Saigon - Sunday' 'Pike-Pine - Weekday'\n",
      " 'Uptown Triangle - Weekday' 'Pioneer Square - Event Day'\n",
      " 'Uptown - Weekday' 'Pioneer Square - Weekday' 'Roosevelt - Weekday'\n",
      " 'Pioneer Square - Sunday' 'South Lake Union - Weekday'\n",
      " 'University District - Weekday' 'Westlake - Weekday' 'Columbia City'\n",
      " '15th Avenue' 'Dexter' 'Lake City' 'Uptown - Sunday' 'West Seattle'\n",
      " 'Commercial Core Saturday' '12th Ave']\n"
     ]
    }
   ],
   "source": [
    "# Group the study areas into more coherent neighborhoods\n",
    "study_areas = df1['Study_Area'].unique()\n",
    "print(study_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_search = ['12th Ave','15th Avenue','Ballard','Belltown',\n",
    "         'Capitol Hill','Cherry Hill','Chinatown','Columbia City',\n",
    "         'Commercial Core','Denny Triangle','Dexter','First Hill',\n",
    "         'Fremont','Green','Lake City','Little Saigon',\n",
    "         'Pike-Pine','Pioneer Square','Roosevelt','SODO',\n",
    "         'South Lake Union','University District','Uptown',\n",
    "         'West Seattle','Westlake']\n",
    "areas = ['12th Ave','15th Ave','Ballard','Belltown',\n",
    "         'Capitol Hill','Cherry Hill','Chinatown','Columbia City',\n",
    "         'Commercial Core','Denny Triangle','South Lake Union','First Hill',\n",
    "         'Fremont','Green Lake','Lake City','Little Saigon',\n",
    "         'Pike-Pine','Pioneer Square','Roosevelt','SODO',\n",
    "         'South Lake Union','University District','Uptown',\n",
    "         'West Seattle','Westlake']\n",
    "area_dict = {}\n",
    "\n",
    "for key in study_areas:\n",
    "    for i,nhood in enumerate(areas_search):\n",
    "        if nhood in key:\n",
    "            area_dict[key] = areas[i]\n",
    "            break\n",
    "            \n",
    "df1['Neighborhood'] = df1['Study_Area'].map(area_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['South Lake Union' '12th Ave' 'SODO' 'Ballard' 'Pioneer Square'\n",
      " 'Cherry Hill' 'Uptown' 'First Hill' 'Commercial Core' 'Fremont'\n",
      " 'Green Lake' 'Capitol Hill' 'Westlake' 'Pike-Pine' 'Belltown'\n",
      " 'University District' 'Roosevelt' 'Chinatown' 'Denny Triangle'\n",
      " 'Little Saigon' 'Columbia City' '15th Ave' 'Lake City' 'West Seattle']\n"
     ]
    }
   ],
   "source": [
    "print(df1['Neighborhood'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make separate columns for time data since we have been having trouble parsing the datetime format\n",
    "date_time = pd.to_datetime(df1['Date Time'])\n",
    "\n",
    "obs_yr = date_time.dt.year\n",
    "obs_month =date_time.dt.month\n",
    "obs_wkday = date_time.dt.weekday\n",
    "obs_hr = date_time.dt.hour\n",
    "obs_qtr = date_time.dt.quarter\n",
    "obs_daytype_dict = {0: 'weekday', 1: 'weekday', 2: 'weekday', 3: 'weekday', 4: 'weekday', 5: 'weekend', 6: 'weekend'}\n",
    "obs_daytype = obs_wkday.map(obs_daytype_dict)\n",
    "\n",
    "df1['Quarter'] = obs_qtr\n",
    "df1['Month'] = obs_month\n",
    "df1['Day of week'] = obs_wkday\n",
    "df1['Weekday or weekend'] = obs_daytype\n",
    "df1['Hour'] = obs_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Elmntkey', 'Neighborhood', 'Study_Area', 'Sub_Area', 'Subarea Label',\n",
       "       'Date Time', 'Study Year', 'Quarter', 'Month', 'Day of week',\n",
       "       'Weekday or weekend', 'Hour', 'Side', 'Unitdesc', 'Peak Hour_SDOT',\n",
       "       'Parking_Spaces', 'Total_Vehicle_Count', 'Utilization', 'Free_Spaces',\n",
       "       'Dp_Count', 'Rpz_Count', 'TG_Car2Go', 'BMW_DN', 'Lime', 'Idling',\n",
       "       'Field Notes', 'Construction', 'Event Closure',\n",
       "       'Peak Hour? (Yes or No)', 'RPZ Blocks', 'CSM', 'Time Stamp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[['Elmntkey', 'Neighborhood', 'Study_Area', 'Sub_Area', 'Subarea Label', 'Date Time', 'Study Year', 'Quarter', 'Month',\n",
    "       'Day of week', 'Weekday or weekend', 'Hour', 'Side', 'Unitdesc',\n",
    "       'Peak Hour_SDOT', 'Parking_Spaces', 'Total_Vehicle_Count', 'Utilization', 'Free_Spaces', 'Dp_Count',\n",
    "       'Rpz_Count', 'TG_Car2Go', 'BMW_DN', 'Lime', 'Idling', 'Field Notes',\n",
    "       'Construction', 'Event Closure',  \n",
    "       'Peak Hour? (Yes or No)', 'RPZ Blocks', 'CSM', 'Time Stamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('Annual_Parking_Study_Data_Cleaned2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
